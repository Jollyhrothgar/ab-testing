{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing and characterizing metrics for experiments\n",
    "- Defining a metric\n",
    "  - Brainstorming and establishing a suite of metrics\n",
    "  - Going from high level concept of a metric to a practical definition that works given a means of data capture\n",
    "- Build intuition about metric\n",
    "  - Understanding sensitivity and robustness\n",
    "- Characterize variability of the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a metric\n",
    "\n",
    "In other words, how do we measure whether the experiment group is better than control group or not? But first, you need to know what you're going to use the metrics for before you decide on how to define them.\n",
    "\n",
    "### Two main use cases\n",
    "- Invariant checking\n",
    "  - Metrics that shouldn't change across your experiment and control. Sanity check that the experiment is run properly.\n",
    "  - E.g. are the populations the same between the experiment and control groups?\n",
    "    - Same number across the two?\n",
    "    - Same distribution? Comparable numbers of users across countries? Comparable numbers of users across languages?\n",
    "- Evaluation\n",
    "  - __High-level business metrics__ (how much revenue you make, what your market share is, how many users you have).\n",
    "  - Then __more detailed metrics__ that focus on the user experience with the product (e.g. how long users stay on the page)\n",
    "  - E.g. Evaluation of why users aren't finishing a video series for a class\n",
    "    - To understand why, we need to dig into the user experience, e.g. the videos could be taking too long to load and we should look at latency\n",
    "  - It's also possible that the business metrics may not work out in the context of an experiment. You may not have the information you need or the time that you're running the experiment may be too short to measure the effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to go about making a metric definition\n",
    "\n",
    "1. First, come up with a high level concept for a metric -- a one sentence summary that everyone can understand, e.g. \"active users\" or \"click-through-probability.\"\n",
    "2. The second step is to figure out the details.\n",
    "  - E.g. how do you define \"active\" in \"active users?\" 7 day active? 28 day active? What events count as activity?\n",
    "3. Summarize all these individual measurements into a single metric (sum, count, average, median).\n",
    "\n",
    "And finally, you have a metric definition that you can use.\n",
    "\n",
    "Should you use a single metric or multiple metrics? For invariant checking, of course, check multiple metrics. For evaluation, however, it depends on the company and general comfort with data. \n",
    "\n",
    "Reasons to use multiple metrics:\n",
    "- Some company leaders may be more comfortable with a suite of metrics where they can see how things move. \n",
    "- Can create a composite metric\n",
    "  - E.g. an objective function or an OEC (overall evaluation criterion) can be a weighted function that outputs a single combined metric that summarizes all the individual metrics (e.g. revenue, users, etc)\n",
    "  - Possibly problematic if you try to overoptimize or need to explain why its moving, and return to talking about individual metrics\n",
    "  \n",
    "Reasons to use a single metric:\n",
    "- On the other hand, for PR or external reporting purposes, you may need to settle on a single overall objective.\n",
    "- Also, if you are a large company, it helps for multiple teams to be aligned on a single metric.\n",
    "\n",
    "Finally, you want to consider how generalizable a metric is. If you're running a whole suite of A/B tests, then ideally you have one or more metrics that you can run across the entire suite. It's better to have a less optimal metric for a test that you can use across the suite of tests for comparison than to have a perfect metric for your test that is not generalizable. More custom = more risk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level metrics for tests\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
